{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwGxxlYJl1_M"
   },
   "source": [
    "# Model Evaluation\n",
    "Dev Note: Try `print` Pandas df if `display` does not work. An error sometimes is thrown if `display`. The issue is due to certain imported libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0dNbitKl1kH"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZOblJ7h4MHR"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    # authorize & import/mount colab/google drive\n",
    "    from google.colab import output\n",
    "    from google.colab import drive\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    # install libraries\n",
    "    !pip install --upgrade openai\n",
    "    !pip install --upgrade numba\n",
    "    !pip install transformers\n",
    "    !pip install sentence_transformers\n",
    "    !pip install unidecode\n",
    "    !pip install bertopic\n",
    "    !pip install unidecode\n",
    "    !pip install pandas==1.1.5\n",
    "\n",
    "    # clear output\n",
    "    output.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiVqKejx2BQT",
    "outputId": "8837ff4c-324a-4a9b-b063-4be94a313b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import openai\n",
    "import nltk\n",
    "import torch\n",
    "import string\n",
    "import gspread\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from io import BytesIO\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# NOTE: For some reason, we get an error of Python 3.7 the first time the notebook \n",
    "# is run on a new machine thorugh Colab. Solution: Remove the bertopic import, run \n",
    "# the cell, paste the bertopic import back to the cell, and then run the cell \n",
    "# again. Don't ask. No fucking idea why.\n",
    "# Note: Removed; not required for model evaluation...\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "key = os.environ['openai_api']\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Alice: /content/drive/MyDrive/w210_Capstone_Project_Fall2021/Repo/\n",
    "# Tim: /content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo\n",
    "ROOT_DIR = \"/content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo\"\n",
    "EVALUATION_EXPORT_DIR = f\"{ROOT_DIR}/memorai/evaluation/reports\"\n",
    "PREDICTIONS_SAVE_DIR = f\"{ROOT_DIR}/memorai/evaluation/predictions\"\n",
    "TEST_SET_SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1xjH4mlfoLqBdQ_WNUYyMrmTmbTm7VA8KiKTjBqom5oM/edit#gid=1838923989\"\n",
    "MODELS_SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1zzzGBn7oyFMDQYQo19Xmec1-qDRFmAB7xsULVTGpy2w/edit#gid=1164311185\"\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "SCORE_PRECISION = 4\n",
    "DEFAULT_ALEX_CONFIGS = {\n",
    "    \"engine_name\": \"curie:ft-brainmonkey-foundation-2021-10-26-08-56-48\",\n",
    "    \"temp\": 0.1,\n",
    "    \"pres_pen\": 1,\n",
    "    \"freq_pen\": 1,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stability_thd\": 0.3,\n",
    "    \"echo\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVwUhZVHv6ZB"
   },
   "source": [
    "## Test Set & Model List Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mypIvsP8vqd_"
   },
   "outputs": [],
   "source": [
    "def load_gsheet(url: str, tab: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Google spreadsheet and return a Pandas DF.\n",
    "    \"\"\"\n",
    "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
    "    worksheet = gc.open_by_url(url).worksheet(tab)\n",
    "    data = worksheet.get_all_values()\n",
    "    headers = data.pop(0)\n",
    "\n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# load test set\n",
    "TEST_DF_ALEX = load_gsheet(TEST_SET_SPREADSHEET_URL, \"alex\")\n",
    "TEST_DF_ALEX = TEST_DF_ALEX.loc[:, [\"Set Symbol\", \"Question\", \"Answer\"]]\n",
    "TEST_DF_ALEX[\"Prediction\"] = None\n",
    "TEST_DF_ALEX[\"F1\"] = None\n",
    "TEST_DF_ALEX[\"Fluency\"] = None\n",
    "TEST_DF_ALEX[\"Relevancy\"] = None\n",
    "TEST_DF_ALEX = TEST_DF_ALEX.astype({\n",
    "                                'F1': 'f', \n",
    "                                'Fluency': 'f', \n",
    "                                'Relevancy': 'f'})\n",
    "# load eval model configs\n",
    "EVAL_MODELS = load_gsheet(MODELS_SPREADSHEET_URL+f'?{random.randint(0, 9999999999)}', \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXu6FfC8lxqz"
   },
   "source": [
    "## GPT-3 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v85M9iDmzgaB"
   },
   "source": [
    "Note: Make sure to turn `echo=False` during evalution so question is not repeated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jb5qYxXl1W4G"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Retrieve BERTopic model from EC2\n",
    "    \"\"\"\n",
    "    load_bert = BERTopic.load('bertopic_trained_alex_1026')\n",
    "    return load_bert\n",
    "\n",
    "def topic_similarity(question):\n",
    "    \"\"\"\n",
    "    Use the trained BERTopic to find out whether the user's question belongs \n",
    "    to the train data topic distribution. \n",
    "    \"\"\"\n",
    "    topic_model = get_model()\n",
    "    question_token = simple_preprocess(question, deacc=True, max_len=512)\n",
    "    question_whole = \" \".join([kept for kept in question_token if not kept in stop])\n",
    "    similar_topics, similarity = topic_model.find_topics(question_whole, top_n=5)\n",
    "    top_score = similarity[0]\n",
    "    return top_score\n",
    "\n",
    "def content_filtering(answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Filter GPT-3 completion before returning to user\n",
    "    If content is sensitive or unsafe, regenerate completion \n",
    "    0 = safe, 1 = senstive, 2 = unsafe\n",
    "    \"\"\"\n",
    "    content_filter = openai.Completion.create(\n",
    "        engine=\"content-filter-alpha\",\n",
    "        prompt= \"<|endoftext|>\"+answer+\"\\n--\\nLabel:\",\n",
    "        temperature=0,\n",
    "        max_tokens=1,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        logprobs=10\n",
    "    )\n",
    "    content_rate = content_filter['choices'][0][\"text\"]\n",
    "    return content_rate\n",
    "\n",
    "def completion(\n",
    "            question: str, \n",
    "            configs: dict = DEFAULT_ALEX_CONFIGS) -> str:\n",
    "    \"\"\"\n",
    "    Generate completion given the question using the params\n",
    "    \"\"\"\n",
    "    # NOTE: Turn off \"echo\" during evaluation!\n",
    "    answer_parse = openai.Completion.create(\n",
    "                                    model = configs['engine_name'],\n",
    "                                    prompt = question,\n",
    "                                    temperature = configs['temp'],\n",
    "                                    max_tokens = configs['max_tokens'],\n",
    "                                    frequency_penalty = configs['freq_pen'],\n",
    "                                    presence_penalty = configs['pres_pen'],\n",
    "                                    echo = configs['echo'],\n",
    "                                    stop = [\" \\###\"])\n",
    "    return answer_parse['choices'][0]['text']\n",
    "\n",
    "# app = FastAPI()\n",
    "# @app.get(\"/alex_gpt/{question}\")\n",
    "def alex_gpt(\n",
    "        question: str, \n",
    "        skip_guardrail: bool = False,\n",
    "        configs: dict = DEFAULT_ALEX_CONFIGS) -> str:\n",
    "    \"\"\"\n",
    "    Receive the question and fine topic of it\n",
    "    Go through content filtering first, if unsafe, refuse to answer\n",
    "    If topic is higher than threshold then answer question\n",
    "    If answer is unsafe, keep generate new answer until safe or sensitive\n",
    "    Return I don't know if the question is lower than threshold\"\n",
    "    \"\"\"\n",
    "    try_times = 3\n",
    "    pronouns = {\n",
    "        \"alex\":\"\",\n",
    "        \" are you\":\" am I\",\n",
    "        \" are you \":\" am I \",\n",
    "        \"Are you \":\"Am I \",\n",
    "        \"You \":\"I \",\n",
    "        \" you \":\" I \",\n",
    "        \" your \":\" my \",\n",
    "        \"Your \":\"My \",\n",
    "        \" me \":\" you \"}\n",
    "\n",
    "    # If there's empty question\n",
    "    if not question or question == \"\":\n",
    "        return \"Ask me a question that you would like to know from me\"\n",
    "\n",
    "    # Parse question from api\n",
    "    question_parsed = \" \".join(question.split(\"_\"))\n",
    "\n",
    "    # Don't take question less than 3 words\n",
    "    if len(question_parsed.split()) <3:\n",
    "      return \"That's not a fully formatted question, is it?\"\n",
    "\n",
    "    # Change pronous, a bit hacky but quick\n",
    "    for key in pronouns.keys():\n",
    "        question_parsed = question_parsed.replace(key, pronouns[key])\n",
    "    \n",
    "    if skip_guardrail:\n",
    "        return completion(question_parsed, configs)\n",
    "\n",
    "    # Content filter question\n",
    "    content_rating_question = content_filtering(question_parsed)\n",
    "    if content_rating_question == \"2\":\n",
    "        return \"Sorry, can't answer that one, that's not very polite.\"\n",
    "    \n",
    "    # Anwer and content filter answer\n",
    "    else:\n",
    "        similarity_score = topic_similarity(question_parsed)\n",
    "        if similarity_score >= SIMILARITY_THRESHOLD: \n",
    "            answer = completion(question_parsed, configs)\n",
    "            content_rate = content_filtering(answer)\n",
    "            cur_rate = content_rate\n",
    "\n",
    "            # Try 3 times if answer is offensive, regenerate answer\n",
    "            while cur_rate == \"2\":\n",
    "                answer = completion(question_parsed, configs)\n",
    "                new_content_rate = content_filtering(answer)\n",
    "                cur_rate = new_content_rate\n",
    "                try_times -= 1\n",
    "                if try_times < 0:\n",
    "                  return \"I have no nice way to respond to this. Try another question.\"\n",
    "\n",
    "            return answer\n",
    "        else:\n",
    "            return \"I really don't know the answer, please try another one.\"\n",
    "\n",
    "def get_alex_configs(\n",
    "            engine_name = DEFAULT_ALEX_CONFIGS['engine_name'],\n",
    "            temp = DEFAULT_ALEX_CONFIGS['temp'],\n",
    "            pres_pen = DEFAULT_ALEX_CONFIGS['pres_pen'],\n",
    "            freq_pen = DEFAULT_ALEX_CONFIGS['freq_pen'],\n",
    "            max_tokens = DEFAULT_ALEX_CONFIGS['max_tokens'],\n",
    "            stability_thd = DEFAULT_ALEX_CONFIGS['stability_thd'],\n",
    "            echo = DEFAULT_ALEX_CONFIGS['echo']) -> dict:\n",
    "    configs = {\n",
    "        \"engine_name\": engine_name,\n",
    "        \"temp\": temp,\n",
    "        \"pres_pen\": pres_pen,\n",
    "        \"freq_pen\": freq_pen,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stability_thd\": stability_thd,\n",
    "        \"echo\": echo}\n",
    "        \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "836TVyTJBk9D",
    "outputId": "625a9b54-a503-4a6c-db59-5f9a310fa503"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:36<00:00, 96.67s/it]\n"
     ]
    }
   ],
   "source": [
    "summary_df = EVAL_MODELS.copy()\n",
    "summary_df['fact_f1'] = None\n",
    "summary_df['inf_fluency'] = None\n",
    "summary_df['inf_relevancy'] = None\n",
    "summary_df['abs_fluency'] = None\n",
    "summary_df['abs_relevancy'] = None\n",
    "summary_df['all_f1'] = None\n",
    "summary_df['all_fluency'] = None\n",
    "summary_df['all_relevancy'] = None\n",
    "\n",
    "# inference\n",
    "test_df_dict = {}\n",
    "for index, row in tqdm(summary_df.iterrows(), total=summary_df.shape[0]):\n",
    "    # sometimes an error is thrown when GPT3's response is too slow; jsut a horrible \n",
    "    # hack to get around the issue \n",
    "    while True:\n",
    "        try:\n",
    "            model_configs = get_alex_configs(\n",
    "                                engine_name = str(row['engine_name']),\n",
    "                                temp = float(row['temp']),\n",
    "                                pres_pen = float(row['pres_pen']),\n",
    "                                freq_pen = float(row['freq_pen']))\n",
    "            test_df_dict[row['model_id']] = test_df = TEST_DF_ALEX.copy()\n",
    "            test_df[\"Prediction\"] = test_df[\"Question\"].apply(lambda x: alex_gpt(x, True, model_configs))\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# save predictions\n",
    "with open(PREDICTIONS_SAVE_DIR+f'/preds - {str(datetime.utcnow())}.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_df_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nka6ltKnmD1I"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agly8aWxfKhG",
    "outputId": "22f30fed-f99b-4466-bc29-bf4abf30e3e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at salesken/query_wellformedness_score were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load fluency model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"salesken/query_wellformedness_score\")\n",
    "model_fluency = AutoModelForSequenceClassification.from_pretrained(\"salesken/query_wellformedness_score\")\n",
    "\n",
    "# load relevancy model\n",
    "# sBERT Doc: https://www.sbert.net/\n",
    "# pre-trained sBERT: https://huggingface.co/sentence-transformers\n",
    "# DistillBERT vs. RoBERTa: https://tinyurl.com/yz27ngb5\n",
    "\n",
    "# STS Benchmark: https://ixa2.si.ehu.eus/stswiki/index.php/STSbenchmark\n",
    "# STS STOA: https://paperswithcode.com/sota/semantic-textual-similarity-on-sts-benchmark\n",
    "\n",
    "# Tested models (ordered from good to bad performance + speed):\n",
    "#   stsb-distilroberta-base-v2\n",
    "#   all-MiniLM-L12-v2\n",
    "#   all-MiniLM-L6-v2\n",
    "#   msmarco-distilbert-cos-v5\n",
    "#   stsb-roberta-large\n",
    "model_se = SentenceTransformer('stsb-distilroberta-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rsDZZO2euYM"
   },
   "outputs": [],
   "source": [
    "def compute_f1(a_pred: str, a_gold: str) -> float:\n",
    "    \"\"\"A modified version of f1 computation from SQuAD.\n",
    "    Ref: https://tinyurl.com/yjscv9oy\n",
    "    \"\"\"\n",
    "    # strip punctuation\n",
    "    a_gold = a_gold.translate(str.maketrans('', '', string.punctuation))\n",
    "    a_pred = a_pred.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # break string into list\n",
    "    gold_toks = a_gold.lower().split(\" \")\n",
    "    pred_toks = a_pred.lower().split(\" \")\n",
    "\n",
    "    # keep prediction tokens only if the token is found in gold_toks \n",
    "    pred_toks = [token for token in gold_toks if token in pred_toks]\n",
    "\n",
    "    # compute f1\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def get_f1_scores(\n",
    "            preds: np.ndarray, \n",
    "            golds: np.ndarray) -> np.ndarray:\n",
    "    if preds is None or len(preds) == 0:\n",
    "        return []\n",
    "\n",
    "    arr = np.vstack([preds, golds]).T\n",
    "    scores_arr = np.apply_along_axis(lambda x: compute_f1(x[0], x[1]), 1, arr)\n",
    "    \n",
    "    return scores_arr\n",
    "\n",
    "def get_fluency_scores(sentences: list) -> np.ndarray:\n",
    "    if sentences is None or len(sentences) == 0:\n",
    "        return []\n",
    "\n",
    "    features = tokenizer(\n",
    "                    sentences, \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    return_tensors=\"pt\")\n",
    "    model_fluency.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model_fluency(**features).logits\n",
    "    \n",
    "    return scores.numpy().flatten()\n",
    "\n",
    "def clean_sentences(sentences: List[str]) -> List[str]:\n",
    "    \"\"\" Convert to unicode characters and strip all punctuations.\n",
    "    \"\"\"\n",
    "    for i, s in enumerate(sentences):\n",
    "        s = unidecode(s)\n",
    "        s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "        sentences[i] = s\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def get_relevancy_scores(\n",
    "                sentences1: list, \n",
    "                sentences2: list,\n",
    "                model: SentenceTransformer = model_se) -> list:\n",
    "    sentences1 = clean_sentences(sentences1)\n",
    "    sentences2 = clean_sentences(sentences2)\n",
    "    \n",
    "    # embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    # compute cosine-similarits\n",
    "    score_matrix = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    scores = []\n",
    "    for i in range(len(sentences1)):\n",
    "        scores.append(float(score_matrix[i][i]))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_GmBxQ2gfBuv",
    "outputId": "00e6b955-6d09-4fbd-8457-86ce6eaee069"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reports Directory:\n",
      "/content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo/memorai/evaluation/reports/2021-12-04 06:14:38.103663\n",
      "\n",
      "Summary Report:\n",
      "/content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo/memorai/evaluation/reports/2021-12-04 06:14:38.103663/summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>all_f1</th>\n",
       "      <th>all_fluency</th>\n",
       "      <th>all_relevancy</th>\n",
       "      <th>all_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99</td>\n",
       "      <td>0.274921</td>\n",
       "      <td>0.55826</td>\n",
       "      <td>0.333737</td>\n",
       "      <td>0.388973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_id    all_f1 all_fluency all_relevancy  all_macro\n",
       "0       99  0.274921     0.55826      0.333737   0.388973"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the report save folder\n",
    "timestamp = str(datetime.utcnow())\n",
    "report_save_dir = f'{EVALUATION_EXPORT_DIR}/{timestamp}'\n",
    "os.mkdir(report_save_dir)\n",
    "\n",
    "# evaluate models\n",
    "for model_id, test_df in tqdm(test_df_dict.items()):\n",
    "    # fact scores\n",
    "    test_df.loc[test_df['Set Symbol'] == 'FACT', 'F1'] = get_f1_scores(\n",
    "                                                            test_df.loc[test_df['Set Symbol'] == 'FACT', 'Prediction'], \n",
    "                                                            test_df.loc[test_df['Set Symbol'] == 'FACT', 'Answer'])\n",
    "    fact_summary_df = test_df.loc[test_df['Set Symbol'] == 'FACT', [\"Set Symbol\", \"F1\"]].groupby([\"Set Symbol\"]).mean()\n",
    "\n",
    "    # fluency scores\n",
    "    scores = get_fluency_scores(test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", \"Prediction\"].tolist())\n",
    "    test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", \"Fluency\"] = scores\n",
    "    fluency_scores_df = test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", [\"Set Symbol\", \"Prediction\", \"Fluency\"]]\n",
    "    fluency_summary_df = fluency_scores_df.loc[:, [\"Set Symbol\", \"Fluency\"]].groupby([\"Set Symbol\"]).mean()\n",
    "\n",
    "    # relevancy scores\n",
    "    scores = get_relevancy_scores(\n",
    "                    test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", \"Answer\"].tolist(),\n",
    "                    test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", \"Prediction\"].tolist())\n",
    "    test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", \"Relevancy\"] = scores\n",
    "    relevancy_scores_df = test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", [\"Set Symbol\", \"Question\", \"Prediction\", \"Relevancy\"]]\n",
    "    relevancy_summary_df = test_df.loc[test_df[\"Set Symbol\"] != \"FACT\", [\"Set Symbol\", \"Relevancy\"]].groupby([\"Set Symbol\"]).mean()\n",
    "\n",
    "    # join & save\n",
    "    model_summary_df = fluency_summary_df.join(relevancy_summary_df, on=\"Set Symbol\")\n",
    "    model_summary_df = model_summary_df.join(fact_summary_df, on=\"Set Symbol\", how=\"outer\").reset_index(drop=True)\n",
    "    test_df.to_csv(f'{report_save_dir}/m{model_id}_full_report.csv')\n",
    "\n",
    "    # update overall summary_df\n",
    "    # f1\n",
    "    df = fact_summary_df.reset_index()\n",
    "    fact_f1 = df.loc[df['Set Symbol'] == 'FACT', 'F1'].values[0]\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'fact_f1']  = fact_f1\n",
    "\n",
    "    # fluency\n",
    "    df = fluency_summary_df.reset_index()\n",
    "    inf_fluency = df.loc[df['Set Symbol'] == 'INF', 'Fluency'].values[0]\n",
    "    abs_fluency = df.loc[df['Set Symbol'] == 'ABS', 'Fluency'].values[0]\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'inf_fluency']  = inf_fluency\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'abs_fluency']  = abs_fluency\n",
    "    \n",
    "    # relevancy\n",
    "    df = relevancy_summary_df.reset_index()\n",
    "    inf_relevancy = df.loc[df['Set Symbol'] == 'INF', 'Relevancy'].values[0]\n",
    "    abs_relevancy = df.loc[df['Set Symbol'] == 'ABS', 'Relevancy'].values[0]\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'inf_relevancy']  = inf_relevancy\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'abs_relevancy']  = abs_relevancy\n",
    "\n",
    "    # macro average scores\n",
    "    all_fluency = (inf_fluency + abs_fluency) / 2\n",
    "    all_relevancy = (inf_relevancy + abs_relevancy) / 2\n",
    "    all_macro = (fact_f1 + all_fluency + all_relevancy) / 3\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'all_f1']  = fact_f1\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'all_fluency']  = all_fluency\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'all_relevancy']  = all_relevancy\n",
    "    summary_df.loc[summary_df['model_id'] == model_id, 'all_macro']  = all_macro\n",
    "\n",
    "# save overall summary report\n",
    "summary_path = f'{report_save_dir}/summary.csv'\n",
    "summary_df.to_csv(summary_path)\n",
    "print(\"\\n\\nReports Directory:\")\n",
    "print(report_save_dir)\n",
    "print(\"\\nSummary Report:\")\n",
    "print(summary_path)\n",
    "\n",
    "# print executive summary\n",
    "df = summary_df[['model_id', 'all_f1', 'all_fluency', 'all_relevancy', 'all_macro']]\n",
    "df.sort_values(by='all_macro', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_evaluation_loop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

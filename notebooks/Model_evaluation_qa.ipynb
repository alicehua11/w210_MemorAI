{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwGxxlYJl1_M"
   },
   "source": [
    "# Model Evaluation\n",
    "Dev Note: Try `print` Pandas df if `display` does not work. An error sometimes is thrown if `display`. The issue is due to certain imported libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0dNbitKl1kH"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 41106,
     "status": "ok",
     "timestamp": 1638598011181,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "cZOblJ7h4MHR"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    # authorize & import/mount colab/google drive\n",
    "    from google.colab import output\n",
    "    from google.colab import drive\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    # install libraries\n",
    "    !pip install --upgrade openai\n",
    "    !pip install --upgrade numba\n",
    "    !pip install transformers\n",
    "    !pip install sentence_transformers\n",
    "    !pip install unidecode\n",
    "    !pip install bertopic\n",
    "    !pip install unidecode\n",
    "    !pip install pandas==1.1.5\n",
    "\n",
    "    # clear output\n",
    "    output.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7416,
     "status": "ok",
     "timestamp": 1638598018593,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "EiVqKejx2BQT",
    "outputId": "462532b8-5633-4fe7-cce9-1623241561c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "import openai\n",
    "import nltk\n",
    "import torch\n",
    "import string\n",
    "import gspread\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from io import BytesIO\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# NOTE: For some reason, we get an error of Python 3.7 the first time the notebook \n",
    "# is run on a new machine thorugh Colab. Solution: Remove the bertopic import, run \n",
    "# the cell, paste the bertopic import back to the cell, and then run the cell \n",
    "# again. Don't ask. No fucking idea why.\n",
    "# Note: Removed; not required for model evaluation...\n",
    "# from bertopic import BERTopic\n",
    "\n",
    "key = os.environ['openai_api']\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Alice: /content/drive/MyDrive/w210_Capstone_Project_Fall2021/Repo/\n",
    "# Tim: /content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo\n",
    "ROOT_DIR = \"/content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo\"\n",
    "EVALUATION_EXPORT_DIR = f\"{ROOT_DIR}/memorai/evaluation/reports\"\n",
    "PREDICTIONS_SAVE_DIR = f\"{ROOT_DIR}/memorai/evaluation/predictions\"\n",
    "TEST_SET_SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1xjH4mlfoLqBdQ_WNUYyMrmTmbTm7VA8KiKTjBqom5oM/edit#gid=1838923989\"\n",
    "MODELS_SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1zzzGBn7oyFMDQYQo19Xmec1-qDRFmAB7xsULVTGpy2w/edit#gid=1164311185\"\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "SCORE_PRECISION = 4\n",
    "DEFAULT_ALEX_CONFIGS = {\n",
    "    \"engine_name\": \"curie:ft-brainmonkey-foundation-2021-10-26-08-56-48\",\n",
    "    \"temp\": 0.1,\n",
    "    \"pres_pen\": 1,\n",
    "    \"freq_pen\": 1,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stability_thd\": 0.3,\n",
    "    \"echo\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVwUhZVHv6ZB"
   },
   "source": [
    "## Test Set & Model List Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1312,
     "status": "ok",
     "timestamp": 1638598019891,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "mypIvsP8vqd_"
   },
   "outputs": [],
   "source": [
    "def load_gsheet(url: str, tab: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Google spreadsheet and return a Pandas DF.\n",
    "    \"\"\"\n",
    "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
    "    worksheet = gc.open_by_url(url).worksheet(tab)\n",
    "    data = worksheet.get_all_values()\n",
    "    headers = data.pop(0)\n",
    "\n",
    "    return pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# load test set\n",
    "TEST_DF_ALEX = load_gsheet(TEST_SET_SPREADSHEET_URL, \"factual_qs_eval\")\n",
    "TEST_DF_ALEX = TEST_DF_ALEX.loc[:, [\"Question\", \"Answer\"]]\n",
    "TEST_DF_ALEX[\"Prediction\"] = None\n",
    "TEST_DF_ALEX[\"F1\"] = None\n",
    "TEST_DF_ALEX = TEST_DF_ALEX.astype({'F1': 'f'})\n",
    "\n",
    "# load eval model configs for gpt3 completition model\n",
    "COMPLETION_MODEL_CONFIGS = load_gsheet(MODELS_SPREADSHEET_URL+f'?{random.randint(0, 9999999999)}', \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXu6FfC8lxqz"
   },
   "source": [
    "## GPT-3 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v85M9iDmzgaB"
   },
   "source": [
    "Note: Make sure to turn `echo=False` during evalution so question is not repeated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1638598019891,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "Jb5qYxXl1W4G"
   },
   "outputs": [],
   "source": [
    "SIMILARITY_THRESHOLD_QUESTION = 0.35\n",
    "SIMILARITY_THRESHOLD_ANSWER = 0.3\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"\n",
    "    Retrieve BERTopic model from EC2\n",
    "    \"\"\"\n",
    "    load_bert = BERTopic.load('bertopic_trained_alex_1026')\n",
    "    return load_bert\n",
    "\n",
    "\n",
    "def topic_similarity(question):\n",
    "    \"\"\"\n",
    "    Use the trained BERTopic to find out whether the user's question belongs \n",
    "    to the train data topic distribution. \n",
    "    \"\"\"\n",
    "    topic_model = get_model()\n",
    "    question_token = simple_preprocess(question, deacc=True, max_len=512)\n",
    "    question_whole = \" \".join([kept for kept in question_token if not kept in stop])\n",
    "    similar_topics, similarity = topic_model.find_topics(question_whole, top_n=5)\n",
    "    top_score = similarity[0]\n",
    "    return top_score\n",
    "\n",
    "\n",
    "def content_filtering(answer: str) -> int:\n",
    "    \"\"\"\n",
    "    Filter GPT-3 completion before returning to user\n",
    "    If content is sensitive or unsafe, regenerate completion \n",
    "    0 = safe, 1 = senstive, 2 = unsafe\n",
    "    \"\"\"\n",
    "    content_filter = openai.Completion.create(\n",
    "        engine=\"content-filter-alpha\",\n",
    "        prompt= \"<|endoftext|>\"+answer+\"\\n--\\nLabel:\",\n",
    "        temperature=0,\n",
    "        max_tokens=1,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        logprobs=10\n",
    "    )\n",
    "    content_rate = content_filter['choices'][0][\"text\"]\n",
    "    return content_rate\n",
    "\n",
    "\n",
    "def question_answer(question: str) -> str:\n",
    "  try:\n",
    "    # best model: temp = 0.4, pres pen = -1.5, freq pen = 2.00\n",
    "    qa_answer = openai.Answer.create (\n",
    "        search_model=\"babbage\", \n",
    "        model=\"davinci\", \n",
    "        question=question, \n",
    "        file=\"file-mcgwAkzglsZSibNyeFuuGjcH\",\n",
    "        examples_context=\"In 2017, U.S. life expectancy was 78.6 years.\", \n",
    "        examples=[[\"What is human life expectancy in the United States?\", \"78 years.\"]], \n",
    "        max_rerank=200,\n",
    "        max_tokens=25,\n",
    "        temperature=0.05,\n",
    "        stop=[\"\\n\", \"<|endoftext|>\"]\n",
    "    )\n",
    "    qa_answer_parse = qa_answer['answers'][0]\n",
    "    # Acccounting for incomplete answer\n",
    "    if not qa_answer_parse.endswith(\".\"):\n",
    "        qa_answer_x = qa_answer_parse.split(\".\")\n",
    "        return qa_answer_x[0]+'.'\n",
    "    else: \n",
    "      return qa_answer_parse\n",
    "  except:\n",
    "    return \"This is not within my training data, I don't have the an answer. Sorry.\"\n",
    "\n",
    "\n",
    "def completion(\n",
    "            question: str, \n",
    "            configs: dict = DEFAULT_ALEX_CONFIGS) -> str:\n",
    "    \"\"\"\n",
    "    Generate completion given the question using the params\n",
    "    \"\"\"\n",
    "    # NOTE: Turn off \"echo\" during evaluation!\n",
    "    answer_parse = openai.Completion.create(\n",
    "                                    model = configs['engine_name'],\n",
    "                                    prompt = question,\n",
    "                                    temperature = configs['temp'],\n",
    "                                    max_tokens = configs['max_tokens'],\n",
    "                                    frequency_penalty = configs['freq_pen'],\n",
    "                                    presence_penalty = configs['pres_pen'],\n",
    "                                    echo = configs['echo'],\n",
    "                                    stop = [\" \\###\"])\n",
    "    return answer_parse['choices'][0]['text']\n",
    "\n",
    "\n",
    "# app = FastAPI()\n",
    "# @app.get(\"/alex_gpt/{question}\")\n",
    "def alex_gpt(\n",
    "        question: str, \n",
    "        skip_guardrail: bool = False,\n",
    "        configs: dict = DEFAULT_ALEX_CONFIGS) -> str:\n",
    "    \"\"\"\n",
    "    Receive the question and fine topic of it\n",
    "    Go through content filtering first, if unsafe, refuse to answer\n",
    "    If topic is higher than threshold then answer question\n",
    "    If answer is unsafe, keep generate new answer until safe or sensitive\n",
    "    Return I don't know if the question is lower than threshold\"\n",
    "    \"\"\"    \n",
    "    return completion(question, configs)      \n",
    "\n",
    "\n",
    "def get_alex_configs(\n",
    "            engine_name = DEFAULT_ALEX_CONFIGS['engine_name'],\n",
    "            temp = DEFAULT_ALEX_CONFIGS['temp'],\n",
    "            pres_pen = DEFAULT_ALEX_CONFIGS['pres_pen'],\n",
    "            freq_pen = DEFAULT_ALEX_CONFIGS['freq_pen'],\n",
    "            max_tokens = DEFAULT_ALEX_CONFIGS['max_tokens'],\n",
    "            stability_thd = DEFAULT_ALEX_CONFIGS['stability_thd'],\n",
    "            echo = DEFAULT_ALEX_CONFIGS['echo']) -> dict:\n",
    "    configs = {\n",
    "        \"engine_name\": engine_name,\n",
    "        \"temp\": temp,\n",
    "        \"pres_pen\": pres_pen,\n",
    "        \"freq_pen\": freq_pen,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stability_thd\": stability_thd,\n",
    "        \"echo\": echo}\n",
    "        \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 84815,
     "status": "ok",
     "timestamp": 1638598104701,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "836TVyTJBk9D"
   },
   "outputs": [],
   "source": [
    "# completion_summary_df = COMPLETION_MODEL_CONFIGS.copy()\n",
    "# completion_summary_df['fact_f1'] = None\n",
    "\n",
    "# inference - q&a\n",
    "qa_test_df = TEST_DF_ALEX.copy()\n",
    "qa_test_df[\"Prediction\"] = qa_test_df[\"Question\"].apply(lambda x: question_answer(x))\n",
    "\n",
    "# # inference - completion\n",
    "# completion_test_df_dict = {}\n",
    "# for index, row in tqdm(completion_summary_df.iterrows(), total=completion_summary_df.shape[0]):\n",
    "#     # sometimes an error is thrown when GPT3's response is too slow; jsut a horrible \n",
    "#     # hack to get around the issue \n",
    "#     while True:\n",
    "#         try:\n",
    "#             model_configs = get_alex_configs(\n",
    "#                                 engine_name = str(row['engine_name']),\n",
    "#                                 temp = float(row['temp']),\n",
    "#                                 pres_pen = float(row['pres_pen']),\n",
    "#                                 freq_pen = float(row['freq_pen']))\n",
    "#             completion_test_df_dict[row['model_id']] = completion_test_df = TEST_DF_ALEX.copy()\n",
    "#             completion_test_df[\"Prediction\"] = completion_test_df[\"Question\"].apply(lambda x: alex_gpt(x, True, model_configs))\n",
    "#             break\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "\n",
    "# # save predictions\n",
    "# with open(PREDICTIONS_SAVE_DIR+f'/preds - {str(datetime.utcnow())}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(completion_test_df_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nka6ltKnmD1I"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638598104703,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "-rsDZZO2euYM"
   },
   "outputs": [],
   "source": [
    "def compute_f1(a_pred: str, a_gold: str) -> float:\n",
    "    \"\"\"A modified version of f1 computation from SQuAD.\n",
    "    Ref: https://tinyurl.com/yjscv9oy\n",
    "    \"\"\"\n",
    "    # strip punctuation\n",
    "    a_gold = a_gold.translate(str.maketrans('', '', string.punctuation))\n",
    "    a_pred = a_pred.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # break string into list\n",
    "    gold_toks = a_gold.lower().split(\" \")\n",
    "    pred_toks = a_pred.lower().split(\" \")\n",
    "\n",
    "    # keep prediction tokens only if the token is found in gold_toks \n",
    "    pred_toks = [token for token in gold_toks if token in pred_toks]\n",
    "\n",
    "    # compute f1\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        #  if either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1\n",
    "\n",
    "def get_f1_scores(\n",
    "            preds: np.ndarray, \n",
    "            golds: np.ndarray) -> np.ndarray:\n",
    "    if preds is None or len(preds) == 0:\n",
    "        return []\n",
    "\n",
    "    arr = np.vstack([preds, golds]).T\n",
    "    scores_arr = np.apply_along_axis(lambda x: compute_f1(x[0], x[1]), 1, arr)\n",
    "    \n",
    "    return scores_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1638598104704,
     "user": {
      "displayName": "Tim Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjCHzDArJyPPAHsYKioqVZeP1reZIsBL9RZDKwCOy8=s64",
      "userId": "17136390159455304905"
     },
     "user_tz": 480
    },
    "id": "_GmBxQ2gfBuv",
    "outputId": "92cf95be-7969-4577-94a4-855f2007ea2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reports Directory:\n",
      "/content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo/memorai/evaluation/reports/2021-12-04 06:08:24.750365\n",
      "\n",
      "Completion Summary Report:\n",
      "/content/gdrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo/memorai/evaluation/reports/2021-12-04 06:08:24.750365/summary_all.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.854422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      qa_f1\n",
       "0  0.854422"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the report save folder\n",
    "timestamp = str(datetime.utcnow())\n",
    "report_save_dir = f'{EVALUATION_EXPORT_DIR}/{timestamp}'\n",
    "os.mkdir(report_save_dir)\n",
    "\n",
    "# evaluate qa model\n",
    "qa_test_df.loc[:, 'F1'] = get_f1_scores(\n",
    "                                qa_test_df.loc[:, 'Prediction'], \n",
    "                                qa_test_df.loc[:, 'Answer'])\n",
    "qa_test_df.to_csv(f'{report_save_dir}/qa_full_report.csv')\n",
    "qa_f1 = qa_test_df.loc[:, \"F1\"].mean()\n",
    "\n",
    "# # evaluate completion models\n",
    "# BEST_COMPLETION_MODEL_ID = '3'\n",
    "# completion_best_model_f1 = 0\n",
    "# for model_id, completion_test_df in tqdm(completion_test_df_dict.items()):\n",
    "#     # fact scores\n",
    "#     completion_test_df.loc[:, 'F1'] = get_f1_scores(\n",
    "#                                         completion_test_df.loc[:, 'Prediction'], \n",
    "#                                         completion_test_df.loc[:, 'Answer'])\n",
    "#     completion_test_df.to_csv(f'{report_save_dir}/completion_m{model_id}_full_report.csv')\n",
    "\n",
    "#     # update completion_summary_df\n",
    "#     fact_f1 = completion_test_df.loc[:, \"F1\"].mean()\n",
    "#     completion_summary_df.loc[completion_summary_df['model_id'] == model_id, 'fact_f1']  = fact_f1\n",
    "\n",
    "#     # track down best model f1\n",
    "#     if model_id == BEST_COMPLETION_MODEL_ID:\n",
    "#         completion_best_model_f1 = fact_f1\n",
    "# completion_f1 = completion_summary_df.loc[:, \"fact_f1\"].mean()\n",
    "\n",
    "# overall summary\n",
    "summary_all_df = {\n",
    "    'qa_f1': [qa_f1]\n",
    "    # 'completion_best_model_f1' : [completion_best_model_f1],\n",
    "    # 'completion_models_macro_avg_f1': [completion_f1]\n",
    "}\n",
    "summary_all_df = pd.DataFrame.from_dict(summary_all_df)\n",
    "summary_all_path = f'{report_save_dir}/summary_all.csv'\n",
    "summary_all_df.to_csv(summary_all_path)\n",
    "print(\"\\n\\nReports Directory:\")\n",
    "print(report_save_dir)\n",
    "print(\"\\nCompletion Summary Report:\")\n",
    "print(summary_all_path)\n",
    "\n",
    "# print executive summary\n",
    "print(\"\")\n",
    "summary_all_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_evaluation_qa.ipynb",
   "provenance": [
    {
     "file_id": "1E5oUrdhv167-EM4iy1Zj7BarCLhHmpEy",
     "timestamp": 1637882978796
    },
    {
     "file_id": "1uuguj_qLF3IK28IMklcILorzfaATu9Ib",
     "timestamp": 1637562980430
    },
    {
     "file_id": "1p8lntCeMw0Vx3-00nSdQ2TGuGFcMtIKr",
     "timestamp": 1636838152091
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

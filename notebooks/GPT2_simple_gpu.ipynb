{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_simple_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  MemorAI - GPT-2 Simple GPU on Alex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rCpzOl8QP9y",
        "outputId": "19b4203d-ed5b-4960-e868-573e3232409b"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "\n",
        "# verify GPU type\n",
        "# NOTE: Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is \n",
        "#       slightly faster than the old K80 for training GPT-2, and has more memory \n",
        "#       allowing you to train the larger GPT-2 models and generate more text.\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Sun Sep 26 03:27:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee30b7cc-9499-4082-9392-41377fe76407"
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# T5 Ref: https://colab.research.google.com/drive/1Sw_1gUPudXw7qBGey-ctSKvY87iFUtGP?authuser=1#scrollTo=g_i7eoklU6KI\n",
        "# GPT2 Simple Ref: https://minimaxir.com/2019/09/howto-gpt2/\n",
        "#GPT2-Simple Git:  https://github.com/minimaxir/gpt-2-simple\n",
        "REPO_PATH = '/drive/MyDrive/MyDrive/Berkeley/W210/w210_Capstone_Project/Repo/memorai/'\n",
        "TRAINING_TXT_FILE = 'alex_tedtalk.txt'\n",
        "\n",
        "# mount Google Drive\n",
        "gpt2.mount_gdrive()\n",
        "gpt2.copy_file_from_gdrive(TRAINING_TXT_FILE) # can't get into any folder path for some unknown reason"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxNOdteNdiIw"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08fd756e-25b2-42ae-a100-c245b452e63a"
      },
      "source": [
        "# download GPT from GCS and saves it to Colaboratory VM at /models/<model_name>.\n",
        "# * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "# * `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "# * `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "# * `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 597Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 6.52Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 662Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 68.0Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 219Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 8.46Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 8.50Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb8327f-2c48-4587-bdda-8ad2b8e83df3"
      },
      "source": [
        "# NOTE: If you want to rerun this cell, restart the VM first (Runtime -> Restart Runtime). \n",
        "#       You will need to rerun imports but not recopy files.\n",
        "# NOTE: GPT2 defaults to a batch size of 1. Each \"example\" defaults to 1024 tokens. \n",
        "#       So a step (i.e. a training step is one gradient update) of GPT2 defaults to 1024 tokens.\n",
        "# DOCs for gpt2.finetune:\n",
        "#       'restore_from': Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "#       'sample_every' Number of steps to print example output\n",
        "#       'print_every' Number of steps to print training progress.\n",
        "#       'learning_rate'  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "#       'run_name' subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "#       'overwrite' Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. \n",
        "RUN_NAME = 'test_run'\n",
        "TF_SESS = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(\n",
        "        TF_SESS,\n",
        "        dataset=TRAINING_TXT_FILE,\n",
        "        model_name='124M',\n",
        "        steps=10,\n",
        "        restore_from='fresh',\n",
        "        run_name=RUN_NAME,\n",
        "        print_every=2,\n",
        "        sample_every=200,\n",
        "        save_every=10)\n",
        "\n",
        "# copy to gdrive\n",
        "gpt2.copy_checkpoint_to_gdrive(run_name=RUN_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3618.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 2747 tokens\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 | 17.21] loss=2.85 avg=2.85\n",
            "[4 | 26.09] loss=2.35 avg=2.60\n",
            "[6 | 34.96] loss=1.73 avg=2.31\n",
            "[8 | 43.84] loss=1.21 avg=2.03\n",
            "[10 | 52.67] loss=1.04 avg=1.83\n",
            "Saving checkpoint/test_run/model-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Model Loading & Inference\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97c0223-59a2-4092-9c4b-3f555b725f64"
      },
      "source": [
        "LOAD_MODEL = True\n",
        "RUN_NAME = 'test_run'\n",
        "\n",
        "if LOAD_MODEL:\n",
        "    # copy the .rar checkpoint file from your Google Drive into the Colaboratory VM\n",
        "    gpt2.copy_checkpoint_from_gdrive(run_name=RUN_NAME)\n",
        "\n",
        "    # NOTE: If you want to rerun this cell, restart the VM first (Runtime -> Restart \n",
        "    #       Runtime). You will need to rerun imports but not recopy files.\n",
        "    TF_SESS = gpt2.start_tf_sess()\n",
        "    gpt2.load_gpt2(TF_SESS, run_name=RUN_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/test_run/model-10\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/test_run/model-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsN2syUvh0k_",
        "outputId": "970ce06e-62bc-4026-f532-73997b432944"
      },
      "source": [
        "# text = gpt2.generate(TF_SESS, return_as_list=True)[0]\n",
        "gpt2.generate(\n",
        "        TF_SESS,\n",
        "        length=50,\n",
        "        temperature=0.5,\n",
        "        prefix=\"Alex loves climbing because\",\n",
        "        nsamples=1,\n",
        "        batch_size=1,\n",
        "        run_name=RUN_NAME)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alex loves climbing because it's so different. It's so different from the way you climb, because you're climbing with a rope. It's so different from the way you walk, because you're walking on a loose ground. It's so different from the way you\n"
          ]
        }
      ]
    }
  ]
}